{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"out_without_nan_mood_normalised.csv\")\n",
    "df['mood_mean_TARGET'] = df['mood_mean_TARGET'].astype(float)\n",
    "df = pd.concat([df,pd.get_dummies(df['id'])],axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['weekDay'],prefix=\"day_\")],axis=1)\n",
    "df = df.drop(['weekDay_time_5','day_time_5','weekDay_time_4','day_time_4','weekDay_time_3','day_time_3','weekDay_time_2','day_time_2','weekDay_time_1','day_time_1','weekDay_time_5','circumplex.valence_mean_time_1'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    del dataset[colname] # deleting the column from the dataset\n",
    "    \n",
    "    return(dataset)\n",
    "newDF =df.drop([\"mood_mean_TARGET\"],axis=1)\n",
    "correlation(newDF,0.5)\n",
    "newDF['mood_mean_TARGET'] = df['mood_mean_TARGET']\n",
    "df = newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df.corr()\n",
    "goodColumns= corr[abs(corr['mood_mean_TARGET'])>0.2]['mood_mean_TARGET'].sort_values()\n",
    "goodColumns = goodColumns.drop(['mood_mean_TARGET'])\n",
    "\n",
    "usedColumns = goodColumns.keys()\n",
    "\n",
    "#usedColumns = ['AS14.01', 'AS14.02', 'AS14.03', 'AS14.05',\n",
    "#       'AS14.06', 'AS14.07', 'AS14.08', 'AS14.09', 'AS14.12', 'AS14.13',\n",
    "#       'AS14.14', 'AS14.15', 'AS14.16', 'AS14.17', 'AS14.19', 'AS14.20',\n",
    "#       'AS14.23', 'AS14.24', 'AS14.25', 'AS14.26', 'AS14.27', 'AS14.28',\n",
    "#       'AS14.29', 'AS14.30', 'AS14.31', 'AS14.32', 'AS14.33', 'day__0.0',\n",
    "#       'day__0.16666666666666666', 'day__0.3333333333333333', 'day__0.5',\n",
    "#       'day__0.6666666666666666', 'day__0.8333333333333334', 'day__1.0','mood_mean','mood_mean_time_2','mood_mean_time_3','circumplex.arousal_mean','circumplex.arousal_mean_time_2','circumplex.arousal_mean_time_3','circumplex.valence_mean','circumplex.valence_mean_time_2','circumplex.valence_mean_time_3']\n",
    "#usedColumns=['mood_mean','mood_mean_time_2','mood_mean_time_3','circumplex.arousal_mean','circumplex.arousal_mean_time_2','circumplex.arousal_mean_time_3','circumplex.valence_mean','circumplex.valence_mean_time_2','circumplex.valence_mean_time_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AS14.07', 'lastTimeSeenMood_7', 'AS14.30',\n",
       "       'numberOfTimesSeenMood5Days_8', 'circumplex.valence_mean',\n",
       "       'mood_mean_time_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usedColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notRelevantAll=df.drop(['id','date'],axis=1).dropna()\n",
    "goldY= notRelevantAll['mood_mean_TARGET']\n",
    "relevant = notRelevantAll.drop(['mood_mean_TARGET'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(usedColumns), input_dim=len(usedColumns), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1042/1042 [==============================] - 0s 240us/step - loss: 41.8443\n",
      "Epoch 2/100\n",
      "1042/1042 [==============================] - 0s 103us/step - loss: 17.2313\n",
      "Epoch 3/100\n",
      "1042/1042 [==============================] - 0s 91us/step - loss: 9.1019\n",
      "Epoch 4/100\n",
      "1042/1042 [==============================] - 0s 91us/step - loss: 5.6204\n",
      "Epoch 5/100\n",
      "1042/1042 [==============================] - 0s 93us/step - loss: 3.1593\n",
      "Epoch 6/100\n",
      "1042/1042 [==============================] - 0s 92us/step - loss: 1.6297\n",
      "Epoch 7/100\n",
      "1042/1042 [==============================] - 0s 94us/step - loss: 0.8786\n",
      "Epoch 8/100\n",
      "1042/1042 [==============================] - 0s 94us/step - loss: 0.6083\n",
      "Epoch 9/100\n",
      "1042/1042 [==============================] - 0s 96us/step - loss: 0.5327\n",
      "Epoch 10/100\n",
      "1042/1042 [==============================] - 0s 92us/step - loss: 0.5164\n",
      "Epoch 11/100\n",
      "1042/1042 [==============================] - 0s 93us/step - loss: 0.5114\n",
      "Epoch 12/100\n",
      "1042/1042 [==============================] - 0s 94us/step - loss: 0.5075\n",
      "Epoch 13/100\n",
      "1042/1042 [==============================] - 0s 91us/step - loss: 0.5044\n",
      "Epoch 14/100\n",
      "1042/1042 [==============================] - 0s 92us/step - loss: 0.5023\n",
      "Epoch 15/100\n",
      "1042/1042 [==============================] - 0s 93us/step - loss: 0.4971\n",
      "Epoch 16/100\n",
      "1042/1042 [==============================] - 0s 93us/step - loss: 0.4934\n",
      "Epoch 17/100\n",
      "1042/1042 [==============================] - 0s 92us/step - loss: 0.4935\n",
      "Epoch 18/100\n",
      "   5/1042 [..............................] - ETA: 0s - loss: 1.1930"
     ]
    }
   ],
   "source": [
    "scoring = {'R2': 'r2',\n",
    "           'Mean squared Error': 'neg_mean_squared_error',\n",
    "           'Explained variance': 'explained_variance'}\n",
    "\n",
    "scores = cross_validate(estimator, relevant[usedColumns], goldY, cv=7,scoring=scoring)\n",
    "print(\"R2: %0.2f (+/- %0.2f)\" % (scores['test_R2'].mean(), scores['test_R2'].std() * 2))\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores['test_Mean squared Error'].mean(), scores['test_Mean squared Error'].std() * 2))\n",
    "print(\"EXPLAINED VARIANCE: %0.2f (+/- %0.2f)\" % (scores['test_Explained variance'].mean(), scores['test_Explained variance'].std() * 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
